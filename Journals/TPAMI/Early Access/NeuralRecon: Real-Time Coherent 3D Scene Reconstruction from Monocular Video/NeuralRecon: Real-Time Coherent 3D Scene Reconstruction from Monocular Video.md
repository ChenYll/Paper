# 摘要
我们提出了一个名为 NeuralRecon 的新框架，用于从单目视频中实时重建三维场景。与先前的方法不同，先前的方法在每个关键帧上分别估计单视图深度图，然后将其融合，我们提出直接通过神经网络顺序地重建每个视频片段的局部表面，表示为稀疏的 TSDF 体积。使用基于门控循环单元（Gated Recurrent Units, GRU）的学习型 TSDF 融合模块来指导网络从先前的片段中融合特征。这种设计允许网络在顺序重建表面时捕获局部平滑先验和全局形状先验，从而实现准确、连贯和实时的表面重建。融合的特征还可以用来预测语义标签，使我们的方法能够同时重建和分割三维场景。此外，我们提出了一种高效的自监督微调方案，通过可微分体积渲染基于输入图像细化场景几何。这种微调方案提高了在微调场景上的重建质量以及对类似测试场景的泛化能力。我们在 ScanNet、7-Scenes 和 Replica 数据集上的实验表明，我们的系统在准确性和速度方面都优于最先进的方法。

# 索引术语
- 三维重建
- 场景理解
- 神经场景表示

# 引言
3D场景重建是3D计算机视觉中的一个核心任务，具有多种应用。例如，在增强现实（AR）中，为了实现AR效果与周围物理场景之间的真实和沉浸式交互，3D重建需要准确、连贯并在实时进行。虽然使用最先进的视觉-惯性SLAM系统[1][2][3]可以准确跟踪相机运动，但由于重建质量低下和高计算需求，基于图像的实时密集重建仍然是一个挑战。大多数基于图像的实时3D重建流程[4][5]采用深度图融合方法，类似于KinectFusion[6]这样的RGB-D重建方法。首先使用如[7][8][9][10]等实时多视图深度估计方法估计每个关键帧的单视图深度图。然后，使用多视图一致性和时间平滑性等标准对估计的深度图进行过滤，并融合到截断有符号距离函数（TSDF）体中。可以使用Marching Cubes算法[11]从融合的TSDF体中提取重建的网格。这种基于深度的流程有两个主要缺点。首先，由于在每个关键帧上单独估计单视图深度图，每次深度估计都是从头开始的，而不是基于之前的估计，即使视图重叠很大。结果，即使相机自我运动是正确的，尺度因子也可能变化。由于不同视图之间的深度不一致性，重建结果容易分层或散乱。图1中红色框中的一个示例是，基于深度的方法在椅子和墙壁上难以产生连贯的深度估计。其次，由于需要在重叠的局部窗口中单独估计关键帧深度图，同一3D表面的几何形状在不同的关键帧中被多次估计，导致冗余计算。在本文中，我们提出了一个名为NeuralRecon的新颖框架，用于实时单目重建，该框架直接在体积TSDF表示中联合重建和融合3D几何形状。给定一系列单目图像及其对应的由SLAM系统估计的相机姿态，NeuralRecon以视图独立的方式递增地重建局部几何形状，而不是视图依赖的深度图。具体来说，它将图像特征反投影以形成3D特征体积，然后使用稀疏卷积处理特征体积以输出稀疏TSDF体积。通过粗到细的设计，预测的TSDF在每个级别上逐渐细化。通过直接重建隐式表面（TSDF），网络能够学习自然3D表面的局部平滑度和全局形状先验。与分别预测每个关键帧的深度图的基于深度的方法不同，NeuralRecon在局部片段窗口内联合预测表面几何形状，因此可以产生局部连贯的几何估计。为了使当前片段重建与之前重建的片段全局一致，我们提出了一个基于门控循环单元（GRU）的学习型TSDF融合模块。GRU融合使当前片段重建依赖于之前重建的全局体积，实现了一种联合重建和融合方法。结果，重建的网格是密集的、准确的，并且在全局尺度上是连贯的。此外，预测体积表示还消除了基于深度方法中的冗余计算，使我们能够使用更大的3D CNN，同时保持实时性能。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/0132a634b49b4dea9db4742401f59ae9.jpeg" width="70%" />
</div>


# 3. 方法
给定由SLAM系统提供的单目图像序列 $\{I_ t\}$ 和相机姿态轨迹 $\{\xi_ t\} \in SE(3)$，我们的目标是准确重建具有语义标签的密集3D场景几何体。为了实现实时推理速度，NeuralRecon（第3.1节）的架构被设计为一个粗到细的过程，共三个层次。在每个层次中，局部视锥体的特征体积融合到全局隐状态以实现一致的重建。为了进一步提高我们系统的泛化能力，我们提出了一个自监督的微调过程（第3.2节），以输入图像作为监督来细化新场景的重建。系统概览见图2。    
<div align=center>
 <img src="https://img-blog.csdnimg.cn/direct/fe8dd4d8b7924ec2b68c35592f30d15f.jpeg" width="70%" />
</div>
## 3.1 NeuralRecon架构
我们表示要重建的全局TSDF体积为 $S^g_ t$，其中 $t$ 表示当前时间步。系统架构见图4。

<div align=center>
<img src="https://img-blog.csdnimg.cn/direct/e9ad82a4b9b64fd49c8adc9e4fd9c590.jpeg" width="70%" />
</div>

### 3.1.1 关键帧选择
为了实现适合交互式应用的实时3D重建，重建过程需要是增量的，并且输入图像应顺序地在局部片段中处理。我们的目标是从传入的图像流中找到一组合适的关键帧作为网络的输入。为了在保持多视图共视性的同时提供足够的运动视差，所选的关键帧既不能太近也不能太远。按照文献[9]，如果新传入帧的相对平移量大于 $t_ {max}$ 且相对旋转角大于 $R_ {max}$，则将其选为关键帧。定义一个包含 $N$ 个关键帧的窗口作为局部片段。选定关键帧后，计算一个包含所有关键帧视锥体的立方体形状的片段边界体积（FBV），在每个视图中固定最大深度范围 $d_ {max}$。在每个片段的重建期间，只考虑FBV内的区域。

### 3.1.2 联合片段重建和融合
我们提出一种基于学习的方法，同时重建局部片段的TSDF体积 $S^l_ t$ 并与全局TSDF体积 $S^g_ t$ 融合。联合重建和融合在局部坐标系中进行。局部和全局坐标系的定义以及FBV的构建见图3。
<div align=center>
<img src="https://img-blog.csdnimg.cn/direct/17c0b0535d4f4ff2974cdb97c500011f.jpeg" width="70%" />
</div>

#### 图像特征体积构建
局部片段中的 $N$ 图像首先通过图像主干网络传递以提取多级特征。类似于先前关于体积重建的工作[13]、[30]、[34]，提取的特征沿每个射线反向投影到3D特征体积中。通过聚合来自不同视图的特征获得图像特征体积 $F^l_ t$。这个反投影过程的可视化见图5 i。
<div align=center>
<img src="https://img-blog.csdnimg.cn/direct/da8517ae81ff47bfa297f8ecbc56b23a.jpeg" width="70%" />
</div>

#### 多视图特征聚合
<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/9a82331f8fb8429eb6ee654c47dc62b7.jpeg" width="70%" />
</div>

受最近工作[32]、[35]的启发，我们提出了一种基于学习的聚合方法，利用变换器（transformer）聚合多视图特征。聚合器的结构见图6a。提出的聚合过程表示为：

$$
f_ i^{att} = \text{ATTENTION}(f^{bp}_ 1, ..., f^{bp}_ N),
$$

$$
w_ i = \text{MLP}(f_ i^{att}),
$$


$$
F = \sum_ {i=1}^{N} w_ i \times f_ i^{att}.
$$

为了简单起见，我们在这里省略了层次索引 $l$ 以及时间索引 $t$，并且只考虑在层次 $l$ 时间 $t$ 的特征体积 $F$ 的一个条目。变换器将反向投影的特征 $\{f_ i^{bp}|i = 1, .., N\}$ 作为无序输入序列，并为每个视图 $i$ 输出关注特征 $f_ i^{att}$。然后，将每个关注特征输入MLP以估计每个源视图的权重 $w_ i$。最终聚合的特征 $F$ 是用权重 $w_ i$ 加权求和的关注特征 $f_ i^{att}$。
#### 粗到细的TSDF重建
我们采用粗到细的方法逐步细化每个层次上预测的TSDF体积。我们使用3D稀疏卷积来有效处理特征体积 $F^l_ t$。 稀疏体积表示也自然地与粗到细的设计相结合。具体来说，TSDF体积 $S^l_ t$ 中的每个体素包含两个值，占用分数 $o$ 和SDF值 $x$。在每个层次上，MLP都会预测 $o$ 和 $x$。占用分数表示体素在TSDF截断距离 $\lambda$ 内的信心。占用分数低于稀疏化阈值 $\theta$ 的体素被定义为空白空间，并将被稀疏化。稀疏TSDF体积的这种表示在图5 iii中有直观的说明。稀疏化后， $S^l_ t$ 通过2倍上采样并与 $F^{l+1}_ t$ 串联作为下个层次中GRU融合模块的输入（稍后介绍）。与为每个关键帧单独预测单视图深度图的基于深度的方法不同，NeuralRecon 在局部片段窗口内联合重建隐式表面。这种设计指导网络直接从训练数据中学习自然表面先验。结果，重建的表面在局部是平滑的，并且在全局上是一致的。值得注意的是，这种设计还导致与基于深度的方法相比计算量大大减少，因为3D表面上的每个区域在片段重建期间只估计一次。
#### GRU融合
为了使片段之间的重建一致，我们提出使当前片段重建依赖于先前片段的重建。我们使用门控循环单元（GRU）[67]的3D卷积变体来实现这一目的。如图5 ii所示，在每个层次中，首先将图像特征体积 $F^l_t$ 通过3D稀疏卷积层传递以提取3D几何特征 $G^l_t$。从全局隐状态 $H^g_{t-1}$ 中提取隐藏状态 $H^l_{t-1}$ 在片段边界体积内。GRU将 $G^l_t$ 与隐藏状态 $H^l_{t-1}$ 融合，并产生更新的隐藏状态 $H^l_t$，该状态将通过MLP层传递以预测该层次的TSDF体积 $S^l_t$。隐藏状态 $H^l_t$ 也将通过直接替换相应的体素更新到全局隐状态 $H^g_t$。正式地，记 $z_t$ 为更新门，$r_t$ 为重置门，$\sigma$ 为sigmoid函数，$W^*$ 为稀疏卷积的权重，GRU将 $G^l_t$ 与隐藏状态 $H^l_{t-1}$ 融合，操作如下：

$$
z_t = \sigma(\text{SparseConv}(\{H^{l}_{t-1}, G^{l}_t\}, W^{z}))
$$

$$
r_ t = \sigma(\text{SparseConv}([H^l_ {t-1}, G^l_ t], W^r)),
$$

$$
\tilde{H}^l_ t = \tanh(\text{SparseConv}([r_ t \odot H^l_ {t-1}, G^l_ t], W^h)),
$$

$$
H^l_ t = (1 - z_ t) \odot H^l_ {t-1} + z_ t \odot \tilde{H}^l_ t.
$$

直观地说，在联合重建和融合TSDF的背景下，GRU中的更新门 $z_ t$ 和遗忘门 $r_ t$ 决定了从先前重建（即隐藏状态 $H^l_ {t-1}$）融合多少信息到当前片段几何特征 $G^l_ t$，以及从当前片段融合多少信息到隐藏状态 $H^l_ t$。作为一种数据驱动的方法，GRU作为一种选择性注意力机制，取代了传统TSDF融合中的线性运行平均操作[6]。在GRU之后预测 $S^l_ t$，MLP网络可以利用从历史片段累积的上下文信息来产生跨局部片段一致的表面几何体。这在概念上也类似于非学习型3D重建流水线中的深度滤波器[4]、[16]，其中当前观测和时间融合的深度与贝叶斯滤波器融合。
#### 全局TSDF体积的集成
在最后一个粗到细层次，预测并进一步稀疏化的 $S^3_ t$ 被转换为 $S^l_ t$。由于在GRU融合中已经完成了 $S^l_ t$ 和 $S^g_ t$ 之间的融合，因此在全局坐标变换后，通过直接替换相应的体素将 $S^l_ t$ 集成到 $S^g_ t$ 中。在每个时间步 $t$，对 $S^g_ t$ 执行Marching Cubes以重建网格。

#### 监督
为了训练网络，我们采用了两种损失函数。占用损失（Occupancy loss）定义为预测的占用值和真实占用值之间的二元交叉熵（Binary Cross-Entropy, BCE）。SDF损失（SDF loss）定义为预测的SDF值和真实SDF值之间的 $\ell_ 1$ 距离。在应用 $\ell_ 1$ 损失之前，我们对预测的SDF值和真实SDF值进行了对数变换。这种监督应用于网络的所有粗到细的层次。

#### 3.1.3 语义分割
除了预测TSDF和占用分数，我们还通过在GRU融合步骤后添加一个单层感知机（MLP），来预测每个体素的语义标签。这使得我们的方法能够在实时条件下同时进行3D场景的重建和分割。为了不降低推理速度，我们采用了一个非常轻量级的MLP来进行语义标签的预测。在ScanNet 3D语义标签基准中，我们采用了包含20个类别的语义标签。在训练过程中，每个体素的真实标签被定义为其最近表面的标签。语义分割损失（Semantic segmentation loss）定义为预测的类别概率和真实标签之间的交叉熵。

#### 3.1.4 实现细节
我们使用了 torchsparse[69] 来实现3D稀疏卷积。图像主干网络是一个修改版的 MnasNet[70]，并且使用在 ImageNet 上预训练的权重进行初始化。在主干网络中，我们采用了特征金字塔网络[71]来提取更具代表性的多级特征。整个网络除了图像主干外，都是端到端训练的，并且除了图像主干外，其余部分的权重是随机初始化的。占用分数 $o$ 是通过一个 Sigmoid 层来预测的。在最后一个层次上，体素的大小为4cm，TSDF截断距离 $\lambda$ 被设置为12cm。 $d_ {max}$ 被设置为3m。$R_ {max}$ 和 $t_ {max}$ 分别被设置为15°和0.1m。稀疏化阈值 $\theta$ 被设置为0.5。在粗到细层次之间的上采样中，我们使用了最近邻插值。

## 3.2 自监督微调
在实际应用中，我们发现当捕获的数据与训练集的条件非常不同时，我们的方法的性能会下降，如图9所示。由于直接在目标场景上训练模型并不现实，因为获取3D地面真实数据非常困难，因此我们提出了一种新颖的自监督微调过程来进一步提升模型在新测试场景下的重建质量。受到最近使用神经辐射场[56]的3D重建方法的启发，我们在预测的稀疏TSDF体积上采用了可微分体积渲染，并在渲染的法线图和RGB图像上应用了监督。这种自监督微调的优势是双重的：首先，它可以作为非实时应用中的测试时优化，以改进当前测试场景的重建质量；其次，微调可以提高模型对新测试场景的泛化能力，尤其是当这些场景与经过微调的场景相似时。我们微调流程的概述见图7。
<div align=center>
 <img src="https://img-blog.csdnimg.cn/direct/945441ae0ed74d75951d06a814429c09.jpeg" width="70%" />
</div>
<div align=center>
<img src="https://img-blog.csdnimg.cn/direct/45474388838c48df8ca4b34a41fbab17.jpeg" width="70%" />
</div>


### 3.2.1 TSDF体积渲染
为了从TSDF体积沿相机 $o$ 发出的射线 $r$ 进行渲染，我们应用了体积渲染技术，该技术在投射的射线上整合值：

$$
P = \sum_ {i=1}^{N} w(i) V(r(t_ i)),
$$

其中 $w(i)$ 和 $V(r(t_ i))$ 分别是在每个样本位置 $r(t_ i)$ 处的权重和值。在我们的情况下，$V(r(t_ i))$ 是 $r(t_ i)$ 处的法线或RGB值。我们遵循NeuS[56]来计算权重 $w(i)$，并使用三线性插值来查询任意非网格位置的TSDF值。在我们稀疏TSDF表示中，只有前一层次的占用估计所定义的表面周围的一个狭窄范围是由网络预测的。每个射线的采样应该只关注这个区域。我们采用了在[72]中提出的表面引导采样策略来生成样本。具体来说，我们首先从TSDF体积中提取网格并在此网格上构建一个八叉树。为了渲染来自相机 $o$ 的射线 $r$，我们查询八叉树以获取表面位置 $t_ s$，然后在范围 $[t_ s - w, t_ s + w]$ 内均匀地绘制 $N$ 个样本，其中 $w$ 是一个预定义的采样范围。我们通过实验发现，使用一个采样范围为8个体素和64个样本可以产生最佳结果。对于法线图的渲染，我们构建了一个法线体积，并且使用插值在任何非网格位置查询法线值。网格上的法线值是作为TSDF体积的一阶导数计算的。最后，我们使用公式(8)，其中 $V(r(t_ i))$ 是插值后的法线，来渲染每个像素的法线。

### 3.2.2 新视角补丁合成
新视角合成需要推断沿射线的每个样本处的辐射度。一种方法是使用MLP来预测辐射度体积，并通过插值在采样位置获得辐射度值。然而，我们发现这种方法既慢又使得几何和辐射度MLP难以收敛。受NeuralWarp[73]的启发，我们不使用单独的MLP，而是使用单射变换将每个样本中心的一个小补丁映射到附近的源视图，并采用双线性插值的RGB作为每个样本的辐射度。然后，每个像素从参考视图可以按照公式(8)渲染，颜色取自源视图。为了处理源视图的遮挡，我们使用从参考视图到TSDF体积中提取的网格的射线投射来查询表面位置。然后，我们计算从源视图到表面的渲染权重。任何从参考视图发出且源视图权重低于阈值 $\lambda_ {occ}$ 的射线都被认为是从源视图遮挡的，并从监督中排除。在我们的实验中，我们将 $\lambda_ {occ}$ 设置为 0.5。

### 3.2.3 训练损失
法线损失（Normal loss）我们采用现成的单视图法线估计方法[74]作为监督。法线损失是渲染和估计法线之间的 $\ell_ 1$ 损失。

$$ \text{Normal loss} = \| \text{Rendered Normals} - \text{Estimated Normals} \|_ 1 $$

RGB损失（RGB loss）我们使用渲染和地面真实补丁之间的归一化交叉相关（NCC）作为RGB损失函数，因为NCC捕获了两个补丁之间的结构相似性。我们只监督那些没有被源视图遮挡的补丁。

$$ \text{RGB loss} = \text{NCC}(\text{Rendered Patches}, \text{Ground Truth Patches}) $$

微调损失（Fine-tuning loss）我们将法线损失和RGB损失组合作为最终的微调损失，并仅对NeuralRecon架构的最后两个层次的输出进行监督。

$$ \text{Fine-tuning loss} = \text{Normal loss} + \text{RGB loss} $$

# 4 实验
在本节中，我们进行了一系列实验来评估NeuralRecon的重建质量和不同设计考虑。实验使用了ScanNet (V2) [77]和7-Scenes [78]两个室内数据集。ScanNet数据集包含1613个室内场景，具有地面真实相机姿态、表面重建和语义分割标签。7-Scenes数据集是另一个具有挑战性的RGB-D数据集，捕获于室内场景。我们使用在ScanNet上训练的模型在7-Scenes上进行验证。

我们使用[13]中提出的3D几何度量和[79]中定义的标准2D深度度量来评估3D重建质量。这些度量的定义在补充材料中有详细说明。在这些3D和2D度量中，我们认为F-score是最合适的度量标准，因为它同时考虑了重建的准确性和完整性。

我们将NeuralRecon与以下基线方法进行比较：实时多视图深度估计方法[7]、[8]、[9]、[18]；多视图立体（MVS）方法[13]、[20]、[23]、[75]、[76]；基于学习的视觉SLAM方法[26]、[28]、[29]；以及隐式神经表示（INR）方法[58]、[59]。在所有这些基线方法中，GPMVS[9]和Atlas[13]分别是最相关的实时和离线方法。

## 4.1 数据集、度量、基线和协议
数据集：我们使用两个室内数据集进行实验，ScanNet (V2) [77]和7-Scenes [78]。ScanNet数据集包含1613个室内场景，具有地面真实相机姿态、表面重建和语义分割标签。7-Scenes数据集是另一个具有挑战性的RGB-D数据集，捕获于室内场景。

度量：3D重建质量使用[13]中提出的3D几何度量和[79]中定义的标准2D深度度量进行评估。

基线：我们的方法与以下基线方法进行比较：实时多视图深度估计方法[7]、[8]、[9]、[18]；多视图立体（MVS）方法[13]、[20]、[23]、[75]、[76]；基于学习的视觉SLAM方法[26]、[28]、[29]；以及隐式神经表示（INR）方法[58]、[59]。

协议：由于我们的方法不显式估计深度图，我们通过渲染重建的网格到图像平面来获得深度图估计[13]。用于评估的关键帧是从视频序列中以10帧的间隔采样的，对于基于深度的方法和Atlas都是如此。[7]、[9]、[23]、[75]在ScanNet上进行微调，遵循[13]、[18]。为了与Atlas进行公平比较，我们还报告了使用双层网格（与Atlas相同）的评估结果。在7-Scenes上的3D几何评估使用单层网格。我们还评估了具有多视图一致性检查的深度过滤操作，详细说明见补充材料。

## 4.2 无微调的NeuralRecon
在本节中，我们评估了不使用微调的实时重建系统，并将其与其他基线方法进行了比较。

### 与ScanNet上的基线方法的比较
使用ScanNet数据集的2D深度度量和3D几何度量进行评估。3D几何评估结果如表1所示。定性结果见  
<div align=center>
<img src="https://img-blog.csdnimg.cn/direct/a533ee8e4414450fa8e68f119e752093.jpeg" width="70%" />
</div>
<div align=center>
<img src="https://img-blog.csdnimg.cn/direct/ca4267cf944e4f80a06b149431931192.jpeg" width="70%" />
  </div>
。我们的方法比最近的基于学习的方法产生更好的性能，并且比COLMAP略好。我们认为改进来自于GRU融合模块实现的联合重建和融合设计。与基于深度的方法相比，NeuralRecon能够局部和全局地产生更连贯的重建。我们的方法在准确性、精确度和F-score方面也超过了体积基准方法Atlas[13]。潜在的改进来自于我们方法中的本地片段分离设计，它可以作为视图选择机制，避免将不相关的图像特征融合到3D体积中。在完整性和召回率方面，所提出的方法与基于深度的方法和Atlas相比性能较差。由于基于深度的方法对每个视图预测像素级深度图，其预测的覆盖率自然很高，但代价是准确性。作为一种离线方法，Atlas在预测几何形状之前具有整个序列的全局上下文，因此Atlas有时甚至比地面真实更完整。然而，Atlas倾向于预测过度平滑的几何形状，完成的区域可能不准确。至于2D深度度量，NeuralRecon在几乎所有2D深度度量方面也优于以前的最先进方法，如表2所示。
<div align=center>
<img src="https://img-blog.csdnimg.cn/direct/6a7991a370c348468ec4b60b622e05a2.jpeg" width="70%" />
</div>
我们还与基于INR的方法进行了比较。定性结果如表3所示。比较的方法使用MLP表示场景为SDF场，并使用单目深度和法线线索进行大规模室内场景重建。它们的方法由于单目线索产生了高视觉质量的网格。

<div align=center>
<img src="https://img-blog.csdnimg.cn/direct/1a0583d7a4294ec6888916a0ae1e6497.jpeg" width="70%" />
</div>

### 7-Scenes上泛化性的评估
在7-Scenes数据集上评估2D深度度量和3D几何度量。如表4所示，我们的方法达到了与最先进方法CNMNet[18]相当的性能，并超过了所有其他方法。由于这里使用的模型仅在ScanNet上训练，这些结果也证明了NeuralRecon能够很好地泛化到训练数据之外的领域。

<div align=center>
<img src="https://img-blog.csdnimg.cn/direct/63f91f24ce644d02ac0fe7b2114fb819.jpeg" width="70%" />
</div>

### 效率
我们还报告了基线方法和我们方法的平均运行时间，如表1所示。仅计算关键帧上的推理时间。NeuralRecon的详细时间分析见表10。对于体积方法（Atlas和我们的方法），运行时间是通过将重建局部片段的TSDF体积的时间除以局部片段中的关键帧数获得的。[8]、[18]、[26]、[29]、[76]和NeuralRecon的运行时间是在NVIDIA RTX 2080Ti GPU上测量的。我们使用[13]和[81]中报告的时间分别为[7]、[9]、[13]、[20]、[75]和[23]测量运行时间。如表1所示，我们的时间成本是每个关键帧34ms，实现了每秒27个关键帧的实时速度，并超过了所有先前的方法。具体来说，我们的方法比Atlas快约9倍，比Consistent Depth快68倍。预测体积表示消除了基于深度的方法中的冗余计算，这有助于我们方法的快速运行速度。与Atlas相比，通过在局部片段中增量重建几何体避免了处理巨大的3D体积，从而比Atlas更快。使用稀疏卷积也有助于提高NeuralRecon的效率。

<div align=center>
<img src="https://img-blog.csdnimg.cn/direct/40ef6472f0084683b7fb1a47a53afddd.jpeg" width="70%" />
</div>
### 语义分割
我们在Scannet 3D语义标签基准上评估语义分割。结果如表5和图8所示。我们的方法可以在许多类别中产生合理的结果。总体IoU较低是因为在观察次数相对较少的类别上表现不佳，例如冰箱、浴帘和水槽。每个类别的定量结果在补充材料中。Atlas[13]以离线模式使用所有帧构建特征体积，而Mix3D[82]和PointTransformerV2[83]使用RGB-D作为输入。我们的在线RGB方法采用了一个非常轻量级的MLP进行语义标签预测，这限制了模型的容量。

<div align=center>
<img src="https://img-blog.csdnimg.cn/direct/fcbd4724f2b047ea83efb73706c600b2.jpeg" width="70%" />
</div>

<div align=center>
<img src="https://img-blog.csdnimg.cn/direct/37d8c92af97446ecacbd9f5a35a16093.jpeg" width="70%" />
</div>

## 4.3 自监督微调
我们使用 Replica [85] 数据集评估我们的微调流程，并遵循 MonoSDF [58] 的相机轨迹。Replica 数据集由 8 个室内场景组成，我们选择 3 个场景作为训练集，其余作为测试集。我们在训练集上运行自监督微调流程，然后在不进行额外微调的情况下对测试集进行推理。微调过程训练了 1.5k 次迭代，学习率为 4e-3。在单个 NVIDIA RTX 3090 GPU 上，对三个训练场景进行微调大约需要 1 小时。我们使用在 Scannet（见第 4.1 节描述）上训练的模型，深度视频 MVS [84] 和 GPMVS [9] 的深度方法，以及 COLMAP [20] 和 VoRTX [32] 的离线方法作为基线。我们在训练场景上的微调定量结果和在测试场景上的推理结果分别在表 6 和表 7 中显示。表 6 显示，与直接网络推理相比，微调能够以超过 40% 的改进重建更准确的 3D 几何图形，并大幅度超越所有其他离线基线。表 7 显示，在训练场景上进行微调也能显著提高网络在类似测试场景上的推理泛化能力。我们在图 9 中展示了微调的定性结果。没有微调，许多平面区域（例如地板）的重建几何体是凹凸不平的，在某些区域完全失败。微调后的结果更平滑，保留了更多细节。这表明所提出的微调方法可以有效地以自监督的方式提高我们方法在新场景下的重建质量。

<div align=center>
 <img src="https://img-blog.csdnimg.cn/direct/8535ef793d084da79578b46e44f1796a.jpeg" width="70%" />
</div>
<div align=center>
 <img src="https://img-blog.csdnimg.cn/direct/0c9bf88dec3c4a9b8942c5be3741680f.jpeg" width="70%" />
</div>

## 4.4 消融研究
在本节中，我们在 ScanNet 数据集上进行了几个消融实验，以验证我们的以下设计选择的有效性，以及在 Replica 数据集上对自监督微调方法的实验。

### 多视图特征聚合
为了评估我们提出的多视图特征聚合器，我们将其与三种非基于学习的聚合方法进行了比较：平均值、归一化交叉相关（NCC）以及将平均值与 NCC 连接。定量结果在表 8 中给出。在我们的工作会议版本中，我们对多视图特征进行了平均，虽然这种简单的策略产生了合理的结果，但它并没有编码多视图一致性，这是 3D 重建的一个重要线索。另一种聚合方法是多视图特征的交叉相关，这在基于学习的 MVS [23]、[24] 中已经广泛使用。但这种方法牺牲了原始图像特征中编码的信息，例如语义先验。连接在非基于学习方法中表现最好，因为它同时考虑了特征本身以及它们在多个视图之间的相关性。上述聚合方法平等对待每个视图，无法处理每个视图的贡献应该不同的情况。我们提出的聚合器优于所有非基于学习方法。我们聚合器中的注意力机制允许网络对每个视图进行不同的处理，从而从多个视图中产生更紧凑的特征。

<div align=center>
<img src="https://img-blog.csdnimg.cn/direct/99d772bbe913406c9eebd9e1746ddb4f.jpeg" width="70%" />
</div>

### GRU融合
我们通过在表 9 中比较从（i）到（iv）的行来验证 GRU 融合设计。为了验证特征融合的好处，我们在表 9 中比较了行（i）和行（ii）。使用特征融合和平均操作比传统的线性 TSDF 融合获得了近 5% 的精度提升。图 10 中的可视化表明，特征融合可以重建更平滑的几何图形。这些结果表明，特征融合比使用相同平均操作的 TSDF 融合更有效。比较表 9 中的行（ii）和行（iii）表明，用 GRU 替换平均操作可以获得 4% 的召回率提升。图 10（iii）中的网格也比图 10（ii）中的更完整。这些结果表明，GRU 更有效地选择性地将当前片段的一致信息整合到隐藏状态中。表 9 中行（iii）和行（iv）的召回率表明，在片段边界体积内进行融合可以产生更完整的结果。图 10（iii）和（iv）中的可视化结果显示，通过在片段边界体积内进行融合，我们的方法在地面上产生了更少的伪影，产生了更一致和完整的表面估计。

<div align=center>
 <img src="https://img-blog.csdnimg.cn/direct/7e4fa636b7fa48c78b7e3f921009ba88.jpeg" width="70%" />
</div>
<div align=center>  
 <img src="https://img-blog.csdnimg.cn/direct/033998bf85f142af9d56b6d638f28e43.jpeg" width="70%" />
</div>

### 自监督微调损失
我们在 Replica 数据集上进行了消融研究，以评估法线和 RGB 监督的影响，如表 6 和表 7 所示。仅使用法线监督，预测的表面更平滑，但缺乏几何约束，导致估计不准确。仅使用 RGB 监督，结果更好，但表面不平滑。结合两种监督会产生最佳结果。

### 体素大小
我们将体素大小设置为 4cm。当体素大小增加到 8cm 时，尽管推理速度降低到 18.6 毫秒，但重建质量显著下降。在 ScanNet 上，精度、召回率和 F 分数分别下降到 0.609、0.393 和 0.476。将体素大小减少到 2cm 是不可行的，因为训练数据无法适应 32G VRAM GPU。因此，4cm 的体素大小在准确性和推理速度之间提供了最佳折衷。

## 5 结论
在本文中，我们介绍了一种新颖的系统 NeuralRecon，用于实时单目视频的 3D 重建。关键思想是通过 3D 稀疏卷积和 GRU 增量地联合重建和融合每个视频片段的稀疏 TSDF 体积。这种设计使 NeuralRecon 能够实时输出准确和连贯的重建，同时进行语义分割。我们还提出了一种自监督微调方法，以在允许测试时优化的情况下提高 NeuralRecon 在新测试场景下的重建质量。实验表明，NeuralRecon 在重建质量和运行速度方面都优于现有最先进方法。
