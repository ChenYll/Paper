# 题目：[Towards Frame Rate Agnostic Multi-object Tracking](https://link.springer.com/article/10.1007/s11263-023-01943-2 )  
## 多目标跟踪的帧率无关性研究
**作者：Weitao Feng, Lei Bai, Yongqiang Yao, Fengwei Yu & Wanli Ouyang** 

****

# 摘要
多目标跟踪（MOT）是计算机视觉领域中最基础的任务之一，对各种视频分析应用有着重要贡献。尽管近期取得了一些有希望的进展，但当前的MOT研究仍然局限于输入流的固定采样帧率。这些研究既不如人类灵活，也与工业场景要求的在复杂条件下对帧率不敏感的跟踪器不完全匹配。实际上，我们通过实验发现，当输入帧率变化时，所有最新的最先进跟踪器的准确性都会急剧下降。为了寻求更智能的跟踪解决方案，我们将研究工作的注意力转移到考虑帧率不敏感性的帧率无关多目标跟踪（FraMOT）问题上。在本文中，我们首次提出了一个带有周期性训练方案（FAPS）的帧率无关MOT框架来解决FraMOT问题。具体来说，我们提出了一个帧率无关关联模块（FAAM），它推断并编码帧率信息以帮助跨多帧率输入的身份匹配，提高了学习模型处理FraMOT中复杂运动-外观关系的能力。此外，由于训练中未包含的后处理步骤在低帧率场景中的差异更大，因此在FraMOT中训练和推理之间的关联差距扩大了。为了解决这个问题，我们提出了周期性训练方案，通过跟踪模式匹配和融合在训练中反映所有后处理步骤。与所提出的方法一起，我们首次尝试为这一新任务建立评估方法。除了提供模拟和评估指标，我们还尝试在两种不同的模式下解决FraMOT任务中的新挑战，即已知帧率和未知帧率，以处理更复杂的情况。在具有挑战性的MOT17/20数据集（FraMOT版本）上的定量实验清楚地证明了所提出的方法可以更好地处理不同的帧率，从而提高了对复杂场景的鲁棒性。

# 关键词
帧率无关 · 多目标跟踪 · 多帧率 · 帧率无关MOT框架 · 帧率信息推断与编码 · 帧率无关关联 · 周期性训练方案

# 1. 引言
多目标跟踪（MOT）是计算机视觉中最基本的任务之一，它帮助机器自动和智能地识别世界，对大量工业视频分析应用做出了贡献。随着深度学习的发展，MOT方法从基于运动-颜色-纹理的跟踪发展到基于运动-外观混合的跟踪，并且与检测任务联合训练，以简化流程和降低计算成本。尽管取得了这些有希望的进展，我们认为目前的MOT算法仍然不够智能，无法很好地满足工业需求，因为它们都是在固定帧率的视频上工作的。
一方面，人类可以在不同帧率的视频上跟踪对象，即使在没有任何关于流帧率信息的情况下。然而，最近的一些最先进的跟踪器在帧率变化时并不具有鲁棒性。例如，我们在MOT17和MOT20挑战中以不同的输入帧率评估了几个最近的跟踪器，并发现当帧率降低时，它们的跟踪能力显著下降。另一方面，不同的应用要求MOT模型在不同的采样率下工作。一些应用（例如，自动驾驶）要求跟踪器具有低延迟并提供非常高采样帧率的输入视频，但更多的应用（例如，大规模轨迹检索和基于跟踪的群体计数）只要求MOT算法在不同的较低帧率下工作。这些应用不太关心在每个时间步长上频繁感知对象位置，而是更关注在更大时间范围内的轨迹完整性。尽管为每个帧率训练和部署单独的跟踪器是可行的，但这种解决方案既不便捷也不高效，因为为每个应用和帧率开发、选择和部署最佳跟踪器对于大型系统来说是费时且成本高昂的。此外，它还假设在测试期间帧率是已知的，这可能并不总是情况。因此，有必要提出能够理解不同帧率视频的跟踪器，这些跟踪器应该是通用的、统一的，并且独立于帧率。这不仅是一个更智能的感知系统的合理目标，也是许多现实世界情况下的高效解决方案，就带宽、存储和计算而言。
为了训练一个统一的帧率无关跟踪器，一个直接的方法是在具有多种不同帧率的数据集上训练一个经典设计的模型（即帧率无关训练）。然而，这种原始设计由于以下两个挑战而效果不佳。首先，不同输入帧率下运动-外观关系的最优匹配规则是不同的。例如，在高帧率输入下，由于相邻帧之间的运动较小，运动线索通常更可靠。而在低帧率设置下，外观线索变得更为重要。在高帧率视频中，即使空间距离较大，具有相似外观的两个检测可能不太可能被判断为同一对象，但在低帧率视频中，它们更有可能被认为是同一对象。传统的关联模型缺乏有效处理这些复杂运动-外观关系的能力。其次，将多帧率数据纳入传统的帧对关联训练方案会导致训练和推理之间差距的扩大。具体来说，训练阶段未包含但在推理阶段应用的后处理步骤将改变检测到的对象位置，导致训练阶段关联网络的输入数据与推理阶段不同。这些变化在正常（更高）帧率下较小，因此在传统的训练方案中可以忽略不计。然而，在低帧率下这些变化被放大，并且在多帧率训练中不可忽视。为了应对这些挑战并实现统一的帧率无关跟踪器，我们提出了一个带有周期性训练方案（FAPS）的帧率无关MOT框架，它包含两种有效的技术。对于第一个挑战，我们提出了一个统一的帧率无关关联模块（FAAM），用于处理各种帧率设置。具体来说，FAAM首先使用可用的帧率信息（例如，确切的帧率）生成帧率嵌入，并推断出帧率感知注意力，然后将其与预测嵌入以通道方式相乘，以预测最终的关联分数。对于测试期间确切帧率未知的情况，我们提出使用帧间最佳匹配距离向量（IBDV）来推断帧率信息。对于第二个挑战，设计了一个周期性训练方案（PTS），通过跟踪模式匹配和融合来增强帧率无关训练。在开始训练之前，我们通过在包含所有后处理步骤的真实推理流程上运行先前的模型检查点来采样跟踪模式。跟踪模式记录了我们在训练期间模拟推理阶段环境所需的所有信息（即位置、运动预测和缓存的特征）。我们假设在短时间内，跟踪器的这些模式变化可以忽略不计，因此我们将整个训练过程划分为几个训练周期，并在周期之间只更新模式。在训练过程中，不匹配这些模式的实例将被丢弃，因为它们可能不会出现在推理时间，从而减少了帧率无关训练的难度。剩余的实例将与记录的模式融合，以减少输入方差，并转化为关联特征。通过所提出的方法，我们成功提高了跟踪器的准确性，特别是在较低帧率设置下。为了对FraMOT任务进行公平评估，我们使用现有的基准MOT数据集模拟多帧率设置，并开发了评估指标。此外，我们还尝试在两种模式下测试FraMOT任务的方法，即已知帧率模式和未知帧率模式，以反映不同的部署需求，并将更具挑战性但实际的MOT问题带给社区。

# 2. MOT应用：背景

## 2.1 场景

**低延迟MOT**  在特定情境下，MOT跟踪器必须以最小的延迟工作，前提是有足够的计算能力可用。这类分类包括可能产生重大结果的自动化评估用例。例如，需要对潜在安全威胁立即响应的实时监控系统，以及必须迅速响应环境变化的自动驾驶车辆。在这些场景中，能够以低延迟执行MOT可以大大增强系统的效率和安全性。

**平衡MOT** 在MOT应用的特定实例中，某些错误可能不会导致灾难性后果。对于这类用例，重要的是应用在同时最大化其处理吞吐量的同时达到令人满意的准确性水平。在这些场景中，最佳方法是在处理速度和准确性之间取得平衡。这通常导致使用低帧率MOT系统。例如，在人群监控应用中，目标可能是跟踪人群的整体运动模式和密度，而不是以完美的轨迹识别每个个体。在交通监控应用中，重点可能是检测交通流量和拥堵，而不是识别每一辆单独的车辆。同样，在仓库或物流应用中，优先考虑的可能是跟踪货物的整体运动，而不是以完美的准确性识别每一个单独的项目。在这些场景中，准确性与高处理吞吐量的结合仍然可以提供有用的洞察力并使有效的决策成为可能。即使对于那些需要详细路径的对象，高感知频率并不总是必要的。例如，在大规模视频监控数据库中，可能只需要每几秒钟确定特定目标的位置，而不是每秒获取25个位置。另一个例子可能是体育分析，其中需要跟踪球员和球的运动，但不需要分析视频中的每一帧，较低的帧率可能足以捕捉比赛的关键时刻。

## 2.2 鲁棒性

**部署鲁棒性** 部署MOT算法是一个具有挑战性的任务，因为跟踪器涉及许多必须为不同用例定制的超参数。帧率是影响这些超参数的关键因素。然而，为不同范围的相机调整大量超参数是不切实际的。为了确保易于大规模部署，我们指的是部署鲁棒性，MOT跟踪器需要表现出更大的智能和适应性，以适应不同的场景、摄像机视角和帧率。

**硬件鲁棒性** 除了部署问题，硬件相关问题也是可能影响MOT算法鲁棒性的重要因素。对于位于数据中心较远的视频收集中心，由于带宽有限或硬件容量过载，网络问题，如丢失数据包和视频延迟，是不可避免的。此外，在某些情况下，可能需要手动配置摄像机以采用不同的采样率，以适应临时的计算中断和恢复。所有这些情况都可能导致视频流中采样帧率的动态变化。对硬件鲁棒性的需求，即解决硬件相关问题，强调了帧率无关MOT算法的重要性。这些算法应该能够在维持其鲁棒跟踪对象的能力的同时，适应由硬件相关问题引起的采样帧率变化。

## 2.3 帧率智能

帧率的变化导致对象运动的变化。处理不同帧率的挑战最终在于管理不同的运动-外观关系。在某些情况下，帧率可能无法准确反映运动分布。例如，高速公路的25 fps视频可能与1 fps的行人视频具有类似的运动分布。在这种情况下，使用运动描述符而不是仅依赖帧率可能更为有效。然而，在某些情况下，运动描述符可能没有意义，例如，当只有少数具有大协方差的运动对象时，确切的帧率数字可能提供精确信息。因此，我们认为应该在两种不同的模式下开发具有帧率智能的MOT解决方案：已知帧率模式和未知帧率模式。这些模式可以解决不同运动-外观关系的问题，并允许更有效的跟踪。

# 4. 帧率无关的MOT框架与周期性训练方案

为了应对FraMOT问题所带来的新兴挑战，有必要开发一种新框架，该框架整合多帧率输入，并具备处理动态运动-外观关系的能力。此外，框架的训练机制应防止由于训练阶段中的非参数后处理步骤，在低帧率设置下导致训练与推理阶段之间差距的扩大。本节介绍了我们提出的带有周期性训练方案（FAPS）的帧率无关MOT框架，该框架专门设计用于解决前述目标并克服FraMOT相关的挑战。

## 4.1 概述

框架中包含三个不同的模块，即联合提取器模块（JEM）、关联模块（AM）和轨迹管理模块（TM）。JEM从原始图像生成检测结果和相应的外观特征嵌入。AM将新的检测结果与现有轨迹关联。TM决定所有轨迹的启动和终止，使它们更平滑，并处理它们的状态。所提出的框架的核心模块是关联模块。我们设计了一个新的帧率无关关联模块，具备编码帧率信息的机制，提供处理各种帧率下复杂运动-外观关系的能力。同时，该框架采用所提出的周期性训练方案（PTS）进行训练，该方案考虑了所有后处理步骤，提供了对推理阶段环境的模拟，从而减少了训练和推理之间数据关联的差距。图2展示了所提出框架的概览。训练管道遵循所提出的PTS，包含多个训练周期，每个周期包含两个阶段，即跟踪模式生成阶段和模块训练阶段。具体来说，在跟踪模式生成阶段，使用上一周期的模型检查点进行前向传递，并生成跟踪模式。跟踪模式包括一些有助于模拟推理运行时的信息（详见4.2节）。在模块训练阶段，关联模型采用JEM的输出和跟踪模式作为输入，生成关联特征，使用所提出的帧率无关关联模块（FAAM）预测关联分数，并受到相应关联真值信号的监督，如图3所示。特别地，在训练期间，不是直接将输入数据传递给FAAM，我们设计了一个关联特征生成模块，通过模式匹配和融合利用生成的跟踪模式调整关联特征。然后，调整后的关联特征将通过FAAM。FAAM网络利用帧率信息推断出帧率感知注意力，并增强关联预测。在推理过程中，将使用上一周期的模型检查点。关联模型仅将JEM的输出作为输入，不再需要跟踪模式，并且移除了模式匹配和融合步骤。推理管道与跟踪模式生成管道相同。

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/638d796fb56e41a8a6fb34933d2997ab.png" width="800" />
</div>

![在这里插入图片描述]()


## 4.2 周期性训练方案

如前所述，FraMOT带给我们的挑战之一是不同的视频帧率扩大了帧对关联训练与实际推理运行时之间的差距。最直接的解决方案是将推理阶段的后处理步骤添加到训练阶段，这很难实现，因为大多数这些步骤不能反向传播。为了解决这个问题，我们提出通过在关联网络中考虑跟踪模式（详见第4.3.1节）来反映推理时的后处理步骤，这是通过周期性训练方案实现的。在这里，跟踪模式是一些可以反映推理时模式的记录，这取决于跟踪器的后处理步骤。本文中，跟踪模式 $P_ t = \{(p_ {loc_ i}, p_ {pred_ i}, p_ {apr_ i}, p_ {lvl_ i})\}$，其中 $i = 1, 2, ..., N_ t$，包括每帧中每个跟踪目标的位置（由 $p_ {loc_ i}$ 表示），基于前一轨迹的Kalman滤波器预测的运动（由 $p_ {pred_ i}$ 表示），推理时保存的外观特征嵌入（由 $p_ {apr_ i}$ 表示），以及两阶段关联策略的级别索引（由 $p_ {lvl_ i}$ 表示），因为我们使用了像一些先前工作一样的级联关联策略（Yu et al., 2016; Zhang et al., 2021）。如果跟踪器有精心设计的遮挡管理，还可以包括其他模式，如轨迹片段的遮挡状态。周期性训练方案的目标是以周期性的方式生成这些跟踪模式，考虑到在短暂的训练周期内，推理阶段的跟踪行为不会发生显著变化。如图2所示，我们设置了多个训练周期，并在每个周期开始前重新采样跟踪模式。在新周期的跟踪模式生成阶段，我们使用上一周期训练好的检查点构建MOT跟踪器，在完整的训练视频上进行跟踪，并生成将在关联特征生成管道中使用的跟踪模式（见第4.3.1节）。然后我们使用训练数据和跟踪模式训练所有模型。当一个训练周期完成后，我们就有了更新的检查点来为下一个周期生成跟踪模式。对于第一个周期，我们使用预训练的联合提取器和基于IoU的手工关联模块来生成模式，而不是使用随机初始化的检查点。例如，在本文中，我们使用带有外观嵌入分支的预训练YOLOX检测框架（Ge et al., 2021）作为JEM，线性组合空间距离和外观相似度作为AM，以及常用的匈牙利算法和带有阈值策略的Kalman滤波器用于目标启动和终止作为TM（详见第4.4节）。对于后续周期，我们使用所提出的帧率无关关联模块（FAAM），而JEM和TM的架构保持不变。整个流程如图1所示。 $L(X, P_ t; N_ t)$ 是整体损失，结合了JEM的检测损失和ID损失，以及AM的关联损失（见第4.3.2节）。 $\lambda_ E$ 和 $\lambda_ A$ 是学习率，可以根据实际需要调整。在我们的实现中， $\lambda_ E$ 设置为0， $\lambda_ A$ 设置为1。

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/4d86143ceaeb40e88fdf15105c6b5df6.png" width="800" />
</div>

![在这里插入图片描述]()



## 4.3 帧率无关关联

关联模型（AM）采用JEM生成的检测结果及其对应的外观特征嵌入，以及PTS提供的跟踪模式（仅限训练）作为输入，并生成由真实标签监督的关联分数。如图3所示，AM有两个部分，即关联特征生成模块和帧率无关关联模块。

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/d9ff1bc03ddb4ae28ca09212c1d03853.png" width="800" />
</div>

![在这里插入图片描述]()


### 4.3.1 关联特征生成

**训练阶段：** 给定帧T和T+ι的帧对，其中ι不是常数（即，采样间隔不固定），JEM生成检测结果 $D_ T = \{B_ {T,i} | i = 1, 2, ..., N_ T\}$， $D_ {T+ι} = \{B_ {T+ι,i} | i = 1, 2, ..., N_ {T+ι}\}$，以及相应的外观特征 $A_ T = \{f_ {T,i} | i = 1, 2, ..., N_ T\}$， $A_ {T+ι} = \{f_ {T+ι,i} | i = 1, 2, ..., N_ {T+ι}\}$，其中 $N_ T$ 表示帧T的建议数量， $B_ {T,i}$ 包含帧T中建议i的边界框和置信度分数， $f_ {T,i}$ 表示帧T中建议i的外观特征向量。同时，我们有来自帧T的跟踪模式，表示为 $P_ T = \{p_ j | j = 1, 2, ..., N_ {p_ T}\}$，其中 $p_ j = \{p_ {loc_ j}, p_ {pred_ j}, p_ {apr_ j}, p_ {lvl_ j}\}$ 由某个边界框位置 $p_ {loc_ j}$、基于推理环境的推理阶段时间预测 $p_ {pred_ j}$ 、推理时保存的外观特征嵌入 $p_ {apr_ j}$ ，以及两阶段关联中匹配级别索引 $p_ {lvl_ j}$ 组成。然后，我们构建了一个模式匹配和融合管道，以模拟推理阶段的跟踪环境，用于帧率无关训练。如图3左部所示，只有与某个 $p_ j$ 匹配的检测才会被选为训练，否则它们将被丢弃，因为它们不会出现在推理阶段（例如，被过滤掉）。在这项工作中，我们定义如果  $p_ j$ 与  $B_ {T,i}$ 的交并比（IoU）在所有可用模式中最大，并且它们的IoU（ $IoU(B_ {T,i}, p_ {loc_ j})$ ）大于一个阈值（例如，我们工作中使用的0.7），则 $B_ {T,i}$ 与模式 $p_ j$ 匹配。然后，匹配的实例与跟踪模式融合，并生成 $D'_ T = \{B_ {T,i} + p_ {pred_ j} | p_ j \in P_ T \land B_ {T,i} \in D_ T \land p_ j \text{ 与 } B_ {T,i} \text{ 匹配}\} \cup \{p_ {loc_ j} + p_ {pred_ j} | p_ j \in P_ T \land p_ j \text{ 未匹配}\}$ 。对应于 $D'_ T$ 的外观特征表示为 $A'_ T$ 。然后， $D'_ T$ 中 $B'_ i$ 的对应嵌入 $f'_ i \in A'_ T$ 为

$$
f'_ i =
\begin{cases}
f_ {T,l} & \text{如果 }f_ {T,l} & \text{如果 } B'_ i \text{ 来自某些 } B_ {T,l}, \\
p_ {apr_ l} & \text{如果 } B'_ i \text{ 来自某些 } p_ {loc_ l}.
\end{cases}
$$

匹配和融合后， $D'_ T$,  $A'_ T$ 和 $D_ {T+ι}$,  $A_ {T+ι}$ 被用来为关联网络计算一些关联特征。我们包括三个距离测量和一个环境变量作为关联特征，表示为 $Z_ {T,T+ι}$，其中

$$
Z_ {T,T+ι} = \{D(B'_ i, B_ {T+ι, j}), IoU(B'_ i, B_ {T+ι, j}), \frac{f'_ i \cdot f_ {T+ι, j}}{|f'_ i| \cdot |f_ {T+ι, j}|}, \text{level}_ i | \forall B'_ i \in D'_ T, B_ {T+ι, j} \in D_ {T+ι}\}.
$$

$D(·)$ 表示归一化的欧几里得距离， $\text{level}_ i$ 是两阶段关联中 $B'_ i$ 出现的匹配级别索引 $p_ {lvl_ j}$。

**推理阶段：** 在推理过程中，不需要将跟踪模式与前一帧的观测结果匹配。匹配的主要目标是模仿推理时的环境，包括后处理。然而，在实际的推理阶段，这种匹配是不必要的。因此，生成关联特征的过程变得更简单，因为可以直接使用跟踪模式：

$$
Z_ {T,T+ι} = \{D(p_ {loc_ i} + p_ {pred_ i}, B_ {T+ι, j}), IoU(p_ {loc_ i} + p_ {pred_ i}, B_ {T+ι, j}), \frac{p_ {apr_ i} \cdot f_ {T+ι, j}}{|p_ {apr_ i}| \cdot |f_ {T+ι, j}|}, \text{level}_ i | \forall p_ i \in P_ T, B_ {T+ι, j} \in D_ {T+ι}\},
$$

其中 $P_ T$ 是从上一帧 T 收集的跟踪模式， $D_ {T+ι}$ 是当前帧 T+ι 的检测结果。换句话说，被跟踪的轨迹可以被视为存储在跟踪模式中。跟踪模式和新观测结果的关联结果正是被跟踪轨迹和新观测结果的关联结果。

### 4.3.2 帧率无关关联模块

关联特征 $Z_ {T,T+ι}$（形状为 $N_ s \times D_ a$， $N_ s$ 是样本数量， $D_ a$ 是每个样本的维度）然后被送入一个4层神经网络 $N_ {aff}$ 并转换为更具区分性的特征 $f_ {aff}$（形状为 $N_ s \times 16D_ a$），如图3的右部分所示。在另一个分支上生成帧率感知注意力。帧率线索首先通过帧率编码器（见下一段）编码成帧率嵌入 $\sigma$，然后该嵌入被池化成形状为 $N_ s \times 32D_ a$，接着通过3层帧率子网络 $N_ {att}$，最后将通道大小减少到与特征分支相同的大小。这个分支的输出表示为 $f_ {att}$（形状为 $N_ s \times 16D_ a$）。关联模块的最终输出预测为

$$
C_ {T,T+ι} = \frac{\sum_ {i} (f_ {aff_ i} \cdot \exp(f_ {att_ i}))}{\sum_ {j} \exp(f_ {att_ j})},
$$

其中 $f_ {aff_ i}$ 表示 $f_ {aff}$ 的第 $i$ 个元素， $f_ {att_ j}$ 表示 $f_ {att}$ 的第 $j$ 个元素。最后，我们对预测 $C_ {T,T+ι}$ 应用二元交叉熵损失 $L_ {assoc}$，对于相同身份的标签为1，不同身份的标签为0。特别地，假检测和真检测的标签为0（它们不是同一身份）；两个假检测的配对将从训练中移除（无法判断它们是否是同一背景区域）。整体损失 $L = L_ {det} + \alpha L_ {id} + \beta L_ {assoc}$，其中 $L_ {det}$ 和 $L_ {id}$ 是联合提取器的检测损失和ID损失，设计类似于Zhang et al. (2021)。

**帧率编码器：** 正如第2.3节所讨论的，帧率在部署阶段可能是已知的或未知的。因此，相应地引入了两种帧率编码器。对于已知的帧率，我们简单地使用给定帧率的余弦嵌入。

具体来说，帧率嵌入 $\sigma_ i = \cos(i \cdot s \cdot F \cdot D_ {\sigma})$，其中 $F$ 是帧率数字， $s$ 是常数缩放因子， $D_ {\sigma}$ 是嵌入维度。

对于未知的帧率，我们提出使用帧间最佳匹配距离向量（IBDV）作为帧率指标。具体来说，IBDV描述了如果实例最佳匹配，两帧中实例之间的归一化距离分布。这个分布大致编码了帧率信息，因为最佳匹配距离在帧率降低时会增加。如图4所示，我们首先通过某些预定义的标准找到最佳匹配对象对，例如，通过最小化空间距离或外观特征距离，然后将这些对的所有归一化距离值放入一个向量中。为了保持其维度不变，我们将向量中的值进行排序，并通过线性插值将它们插值到固定形状。我们在5.4.5节中研究了不同标准查找最佳匹配对的效果。在我们最终的解决方案中，根据MOT数据集上的结果，我们选择“空间距离”作为我们的标准，即我们选择最小化总空间距离的对作为我们的最佳匹配对。数学上，当帧率未知时，

$$
\sigma = \text{interp}(\text{sort}\{D(B'_ i, B_ {T+ι, j}), i = 1, 2, ..., N'\}).
$$

其中 $B'_ i, B_ {T+ι, j}$ 是第 $i$ 对匹配对， $\text{interp}$ 表示线性插值， $\text{sort}$ 表示对向量的值进行非减排序。

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/c4b3920ef3f144069beba4a71a31d997.png" width="800" />
</div>

![在这里插入图片描述]()


## 4.4 在线跟踪

在线跟踪方面，我们采用了一个简单但有效的流水线，包括检测过滤、基于Kalman滤波器的运动预测和两阶段关联策略。目标管理周期在过程2中说明。我们首先使用阈值 $\lambda_ {low}$ 过滤掉背景对象，然后使用Kalman滤波器预测每个被跟踪目标的运动。接下来，我们执行两个不同阶段的关联，这是一种常用的技术，用于减少碎片（Yu et al., 2016; Zhang et al., 2021）。在第一阶段，只有置信度较高的检测结果（置信度分数大于 $\lambda_ {high}$）才会被放入匹配池中。在第二阶段，剩余未匹配的被跟踪目标将再次放入匹配池中，并与不太自信的检测结果（置信度分数小于 $\lambda_ {high}$ 但大于 $\lambda_ {low}$）进行匹配。关联步骤将找出最佳的匹配对，最小化配对距离的总和（或最大化配对分数的总和）。这可以通过匈牙利算法轻松计算。最后，剩余置信度较高的未匹配检测结果（置信度分数大于 $\lambda_ {high}$）将被添加到被跟踪目标列表中。如果跟踪目标列表中的目标在长时间间隔 $\lambda_ i$ 内未匹配，则将其从列表中删除。对于在线跟踪，我们采用了一个简单但有效的处理流程，该流程包括以下步骤：检测过滤、基于Kalman滤波器的运动预测，以及两阶段的关联策略。目标管理周期在过程2中有所描述。我们首先使用一个阈值 $\lambda_ {low}$ 来过滤掉背景对象，然后对每个被跟踪的目标使用Kalman滤波器来预测其运动。接下来，我们执行两个不同阶段的关联，这是一种常用的技术，用于减少跟踪过程中的碎片化问题。

在第一阶段的关联中，只有置信度较高的检测结果（置信度分数大于 $\lambda_ {high}$）会被放入匹配池中。在第二阶段，剩余未匹配的被跟踪目标将再次被放入匹配池，并与置信度较低的检测结果（置信度分数小于 $\lambda_ {high}$ 但大于 $\lambda_ {low}$）进行匹配。关联步骤将找出最佳的匹配对，通过最小化配对距离的总和（或最大化配对分数的总和）。这一过程可以通过匈牙利算法（Hungarian Algorithm）来高效实现。最后，剩余的置信度较高的未匹配检测结果（置信度分数大于 $\lambda_ {high}$）将被添加到被跟踪目标的列表中。如果在跟踪目标列表中的目标在一定时间间隔 $\lambda_ {i}$ 内未匹配，则它将从列表中被移除。

# 5 实验
在本节中，我们提供了定量实验结果、消融研究和对我们提出的解决FraMOT问题的方案的深入分析。

## 5.1 评估数据集和指标
### 5.1.1 数据集
为了评估FraMOT，需要一个多帧率数据集来进行训练和评估算法。我们不是收集和重新注释额外的数据，而是从现有的高帧率MOT数据集（例如MOT17/20）模拟多帧率输入。我们不考虑超过30 fps的更高帧率，因为i）当前大多数摄像机以30 fps的帧率工作，高帧率的流设备很少见，ii）为了降低计算成本，大多数工业场景倾向于降低帧率而不是提高帧率，iii）更高的帧率通常不会带来额外的挑战。给定每秒F帧的视频，总共有N帧，由I1, I2, ..., IN表示。目标帧率是每秒F'帧（F' < F），我们假设F可以被F'整除。我们的模拟方法重新采样原始视频，并将帧重新组合成满足目标帧率的新视频。具体来说，我们生成k = F/F'个新视频，其中第i个视频由帧ˆIi j组成，j = 1, 2, ..., N/k，其中ˆIi j = I(j-1)k+i。我们称k为模拟的采样因子。如图5所示，原始视频被分解成一个k行N/k列的矩阵，每行生成一个新视频，目标帧率为F/k。考虑到大多数视频现在有一个默认的25 fps的帧率，我们从25 fps开始，并采样12.5 fps, 6.25 fps, 3.125 fps, 1.5625 fps, 1 fps, 0.722 fps和0.5 fps的视频，对跟踪器在不同帧率下的性能进行全面评估。这意味着我们在方程7中选择1, 2, 4, 8, 16, 25, 36和50作为k，总共有八种不同的设置。通过使用更广泛的非几何加速一致性，可以在保持较小的评估时间成本的同时，获得更全面的总体影响表示。尽管可以采样帧率低于0.5 fps的视频，但0.25 fps的设置将生成少于10帧的视频，导致测试案例较弱。这个范围也可以根据未来需求的变化进行扩展。表1显示了模拟数据集的统计信息。原始数据集中的每一帧都完美地包含在新数据集中。较低的目标帧率设置结果在更多的视频中，但每个生成视频的长度更短。然而，每个视频的平均不同身份数量保持稳定。

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/ff07f6a4597f4f0badb2b252f118653b.png" width="800" />
</div>

![在这里插入图片描述]()


<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/2e459a59ed6c4f7f8e73a8c19043f78c.png" width="800" />
</div>

![在这里插入图片描述]()

### 5.1.2 指标
遵循CLEAR MOT指标和HOTA指标，我们定义了相应的平均指标，以反映在不同帧率视频上的跟踪器的总体性能。具体来说，我们使用平均HOTA（mHOTA）和平均MOTA（mMOTA）作为帧率不敏感跟踪性能的主要指标，其中

$$ mHOTA = \frac{1}{|Fall|} \sum_ {F' \in Fall} HOTAF' $$

$$ mMOTA = \frac{1}{|Fall|} \sum_ {F' \in Fall} MOTAF' $$

Fall是所有目标帧率的集合。我们还提供平均IDF1（mIDF1）作为主要结果的参考。同样地，mIDF1 = 1/|Fall| ∑F'∈Fall IDF1F。为了进一步调查跟踪器对多个帧率的鲁棒性，我们进一步提出了一个新的指标，称为“脆弱比率”（VR）：

$$ VR = \frac{HOTA_ {highest} - HOTA_ {lowest}}{HOTA_ {highest}} $$

其中HOTAhighest和HOTALowest分别表示所有不同帧率设置中最高的HOTA和最低的HOTA。VR表示在极端情况下可能的最大下降幅度，这将有助于可靠性和有效性评估。

## 5.2 实现细节
**联合提取器** 我们采用YOLOX（Ge et al., 2021）检测框架，加上一个额外的身份分支作为联合提取器模型。骨架类型是yolox-x。实验中所有的基线都使用相同的联合提取器架构，包括检测分支和身份分支（用于跟踪）。

**训练数据** 联合提取器首先在没有身份分支的COCO数据集上预训练，然后在MOT17/20（Milan et al., 2016）、CrowdHuman（Shao et al., 2018）、CityScapes（Cordts et al., 2016）和HIE（Lin et al., 2020）数据集上进行训练。对于MOT数据集，我们将数据集分成两个不同的部分，彼此之间没有重叠。第一部分用于训练，占据了原始数据集的大约60%，第二部分用于评估和消融研究。训练部分进一步分为两组，即A集和B集，它们共享一些帧但也有自己独立的数据。A集和其他非MOT数据集用于联合提取器训练，B集用于联合提取器微调（较小的学习率）和关联模块训练。B集从每个视频中采样300张图像，其中200张图像与A集共享。A集和B集的设计有助于避免关联模块过度拟合到看到的数据。

**数据增强** 我们在非跟踪数据上遵循YOLOX的相同数据增强技术（身份分支和关联模块不会在这些数据上进行优化）。在跟踪数据上，我们只应用随机翻转和随机调整大小，并删除YOLOX中的其他增强。

**优化** 随机梯度下降（SGD）被用作我们的优化器。我们首先在A集和其他非MOT数据集上，不包括关联模块，训练联合提取器60个周期。在这个阶段，我们应用余弦学习率策略，初始学习率为0.00001。批量大小设置为32。然后，我们在B集上微调联合提取器，并按照提出的周期性训练方案（PTS）训练关联模块10个周期每个时期。周期数Np设置为3。

**其他超参数** 在训练阶段，损失权重α设置为0.5，β设置为1。在线跟踪期间，λhigh设置为0.6，λlow设置为0.1。丢弃间隔λi设置为30。如果关联分数大于0.1，则只形成匹配对。

## 5.3 结果
表2显示了最近的开源最先进MOT方法和我们的方法在具有挑战性的MOT17和MOT20数据集上的整体结果，以及前述的多种帧率模拟。ByteTrack（Zhang et al., 2022）、FairMOT（Zhang et al., 2021）和GSDT（Wang et al., 2021）有使用提供的帧率来控制缓存长度的设计策略。CenterTrack和TraDeS没有设计任何与帧率相关的程序，因此它们已知帧率模式的结果缺失。我们的方法在所有指标上都优于其他方法，无论是在已知帧率模式还是未知帧率模式下的帧率不敏感跟踪。在已知帧率模式中，我们在MOT17和MOT20中的mHOTA分别比亚军方法高出4.5%和6.8%。对于mMOTA，我们分别高出5.6%和10.7%。对于mIDF1，我们分别高出6.0%和6.6%。此外，我们有最小的脆弱率（VR），这意味着我们的表现受帧率变化的影响最小。在未知帧率模式下，我们的方法仍然优于其他方法。这种模式下的改进比已知帧率模式小，因为获得准确的运动分布更加困难。我们在MOT17和MOT20的mHOTA方面分别提高了2.8%和5.1%。我们在这种模式下也有更高的mHOTA和mIDF1分数，并且脆弱率也低于其他方法。结果表明，我们提出的方法对帧率不敏感跟踪是有效的。图6进一步显示了这些方法在MOT20 FraMOT模拟数据集上的HOTA和MOTA曲线，关于采样因子k（较大的k表示较低的帧率）。在这些方法中，ByteTrack是第一个使用YOLO-X检测基线的方法，因此具有更准确的检测结果，在正常帧率设置（k = 1）中优于所有其他跟踪器。为了公平比较ByteTrack，我们还基于YOLO-X基线开发了我们的方法。然而，ByteTrack没有使用任何外观特征进行跟踪，而我们的方法有一个额外的跟踪分支来利用外观特征。正如图6所观察到的，我们的方法在k ≤ 4时的HOTA方面略低于ByteTrack，这证实了在高帧率下，外观特征并不能提高跟踪性能，甚至可能损害跟踪性能。然而，当帧率较低时（例如，较大的k），ByteTrack的性能显著下降，因为运动不太可靠。与此同时，我们方法的性能也缓慢下降，但远高于所有比较方法。它表明，当帧率降低时，跟踪变得更加困难，但我们的方法有更好的能力来处理这些复杂的情况。尽管如此，我们方法的整体性能是最好的，这揭示了我们设计的FAAM和PTS训练对FraMOT是有效的。除了MOTChallenge基准之外，我们还使用最近的SOMPT22数据集（Simsek et al., 2023）进行了实验。表3显示了一些最近的SOTA方法和我们的方法在SOMPT22验证集上的性能，未知帧率模式。SOMPT22由训练集的9个视频和测试集的5个视频组成。这个新数据集已经创建，以增强具有中等密度视频和静态摄像机视图的跟踪注释。与MOT20相比，SOMPT22也包括静态摄像机视图，涵盖了更广泛的场景范围，具有各种物体运动。所有方法在MOT20上使用了相同的联合提取器配置和检查点进行测试。我们已经使用了与MOT17/20分割相同的方法来划分SOMPT22训练集并创建验证集。报告的结果已在验证集上进行了测试。从表3中可以看出，我们的方法继续优于最近的SOTA方法。与ByteTrack方法相比，我们在mHOTA、mMOTA和mIDF1方面分别提高了4.1%、9.4%和6.4%。值得注意的是，ByteTrack和我们的方法都有一个更深的骨架。因此，当考虑到一个与以往场景大不相同的新数据集时，这两种方法的性能更好，而FairMOT和GSDT在将它们的检测和跟踪能力转移到新环境中的能力较弱。

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/cc71fa93b21247dcb6343ecafa05915d.png" width="800" />
</div>

![在这里插入图片描述]()

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/564f871c195d4fb080a15ecf276f3d41.png" width="800" />
</div>

![在这里插入图片描述]()

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/f2f133220e9d4cbdb09fc8e75bf556a0.png" width="800" />
</div>

![在这里插入图片描述]()

## 5.4 消融研究和分析
为了更好地理解提出的方法，我们进一步在MOT20验证集（从原始训练集中分割出来，不包括在实际训练集中）上进行了定量实验。

### 5.4.1 提出方法的效果
表4显示了不同组件如何影响整体性能。在这个实验中，我们首先介绍了两种直接的基线方法来解决FraMOT问题，即统一模型（UM）和多个模型（MM）。基线（UM）是一个简单的模型，它直接从所有不同帧率的采样帧对中训练，没有采用本文提出的任何额外策略。相比之下，基线（MM）针对3个不同的采样因子k范围训练了3个独立的模型，即高帧率（k < 6）、中等帧率（6 ≤ k < 20）和低帧率（20 ≤ k），并在测试期间独立部署每个特定于帧率的模型。第一个基线（UM）在推理期间不需要帧率数字，因此在未知帧率模式中进行了介绍。第二个基线（MM）需要帧率数字进行模型切换，因此其结果在已知帧率模式中进行了介绍。所有方法在这项消融研究中都使用了相同的联合提取器框架和检查点，差异在关联模块中。UM基线的平均HOTA、平均MOTA和脆弱比率在表中所有设置中是最差的，表明传统的统一关联模型无法执行帧率不敏感的跟踪。MM基线比统一模型基线更好，因为有了帧率划分的先验知识。然而，结果仍然不尽人意，当帧率数字未给出时，该方法不可行，这与人类跟踪对象的方式不可比。我们提出的方法分别由“UM + PTS”、“FAAM”和“FAAM + PTS”呈现。“UM + PTS”意味着在统一模型基线上应用周期性训练方案。在PTS策略的帮助下，统一模型基线在mHOTA上提高了2.1%，在mMOTA上提高了1.0%，并将脆弱比率降低了8.9%，表明PTS训练成功地减少了帧率不敏感跟踪训练的难度。“FAAM”意味着只应用帧率不敏感关联模块，它可以在已知帧率模式和未知帧率模式下工作。这种统一方法的性能优于两个基线，同时使用的参数量只有MM基线的1/3。更重要的是，它可以自动推断帧率信息。当我们同时应用这两种提出的方法时，呈现为“FAAM + PTS”，我们获得了61.1% mHOTA和77.6% mMOTA的最佳结果，以及24.9%的最低脆弱比率（已知帧率模式）。结果在已知帧率模式中也有所提高，并且与已知帧率模式相当有竞争力。这些结果表明，这两种方法都是有效的，提高了整体帧率不敏感跟踪的准确性，使跟踪器更智能。

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/01da3cf1ae1d4e47b51e1aba501863c2.png" width="800" />
</div>

![在这里插入图片描述]()

### 5.4.2 在未见过的帧率下的结果
我们在训练和测试数据集模拟中选择了8个不同的采样因子k。为了更好地理解所提出方法在处理各种帧率时的行为，我们在未见过的帧率下进行了实验，即在训练集不同的帧率上进行测试。具体来说，我们选择了k = 1, 3, 6, 12, 21, 30, 43, 62，并基于这些未见过的k重建了验证集。我们仍然在未见过的集合中包括正常帧率，即k = 1，因为它是分析的基线。表5显示了两个基线和我们的FAAM配备PTS训练的结果。所有模型在未见过的帧率下都略有下降。然而，我们的FAAM仍然优于两个基线。请注意，这里的FAAM结果是来自未知帧率模式，不需要额外的帧率信息，而MM基线必须在测试期间提供帧率数字。

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/0ae90d40c2074692978aad867f20f531.png" width="800" />
</div>

![在这里插入图片描述]()

### 5.4.3 在动态采样设置下的结果
我们还尝试在动态采样视频上测试这些方法，以模拟一些网络不稳定的场景，其中一些图像会意外地、随机地丢失。我们通过在原始视频中随机采样帧来生成动态采样视频。给定一个有n帧的视频和一个整数k，我们从帧池（最初包括给定视频的所有帧）中随机选择n/k帧，并将所选帧组合成一个视频。然后从帧池中移除所选帧。我们不断从帧池中随机选择帧并生成视频，直到帧池为空。这样，我们将获得总共k个视频，每个生成的视频大约有n/k帧。我们选择了k = 16。图7显示了采样间隔的分布，平均间隔时间为15.7帧（628毫秒），中位间隔时间为11帧（440毫秒）。表6显示了两个基线和我们的FAAM配备PTS训练的结果。在动态采样设置下，我们的FAAM仍然优于两个基线。请注意，这里的FAAM结果是来自未知帧率模式，而基线（MM）将所有动态视频视为1.6 fps（因为平均间隔时间是628毫秒，大约是1.6 fps）。

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/1273534198cf4a9c89ddae67f2d54c7a.png" width="800" />
</div>

![在这里插入图片描述]()

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/92c0f75371ed4be0b63232c0a09983f5.png" width="800" />
</div>

![在这里插入图片描述]()


### 5.4.4 不同运动的影响
为了研究我们提出的方法在不同场景下不同物体运动的有效性，我们进行了额外的实验。为了做到这一点，我们使用了MOT17验证集，并将每个视频序列根据每个视频中存在的物体运动分成三个不同的子集。为了估计物体运动，我们计算了顶部30%最快物体的每帧运动中位数（以像素为单位，ppf），这为视频之间的运动差异提供了清晰的表示。第一个子集由MOT17-02和MOT17-04序列组成，具有静态摄像机视图和慢速物体运动（顶部30%最快物体的中位运动小于4 ppf）。第二个子集包括MOT17-05、MOT17-09、MOT17-10和MOT17-11，具有移动摄像机视图和正常物体运动（顶部30%最快物体的中位运动为8-14 ppf）。第三个子集包括MOT17-13，具有显著转动的摄像机视图和快速物体运动（顶部30%最快物体的中位运动为19 ppf）。我们在每个子集上评估了两种基线方法和我们提出的方法，并记录了结果。

表7显示了我们的发现。基线方法MM在慢速场景下表现更好，但在快速物体运动下效果不佳，而基线方法UM在快速物体运动下表现更好，但在慢速物体运动下表现不佳。相比之下，我们提出的方法在所有三种场景中都保持了平衡的性能，并实现了最高的整体性能。我们观察到，我们的方法在快速子集（MOT17-13）中的表现并不比基线（UM）更好。性能差异可能归因于在视野中包含的物体数量显著减少的延长时间间隔。结果，IBDV（帧间最佳匹配距离向量）特征表示的质量受到损害，导致子集性能下降。统计分析表明，在快速子集中，大约4.4%的帧包含少于五个物体，而在其他序列中，这一比例不到1%。这些帧中物体的稀缺性对IBDV特征的鲁棒性产生了负面影响。然而，由于这些情况下物体的数量较少，它们对整体性能的影响仍然相对次要。因此，观察到的性能下降可能归因于物体数量的减少，而不是运动的增加。尽管如此，必须承认快速运动可能导致可观察物体数量的减少，从而影响整体性能。

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/6417bf43f777415ab9c22b53fc93e5fb.png" width="800" />
</div>




### 5.4.5 超参数的影响
表8显示了超参数如何影响整体性能。在PTS训练中，我们引入了周期数Np。为了理解它的影响，我们进行了不同Np从0到4的实验。0表示未应用PTS。我们可以看到，当Np = 3时，我们获得了最佳性能，而其他Np设置也比未应用的情况获得了更好的性能。结果表明，较大的Np并没有带来更多的增益。这也表明后期不同检查点的推理环境变化不大，关键变化在第一个周期。阈值截断有时极大地影响结果。为了确保匹配阈值稳定，并且增益不是来自阈值策略，我们进行了不同阈值从0.5到1.3的实验。我们可以看到，我们的方法在阈值为1.3时获得最佳mHOTA，在阈值为0.5时获得最佳mMOTA。尽管阈值不同，性能仍然稳定，差异小于0.3%。我们选择0.9作为最终阈值，因为在阈值为0.9时，我们可以在mHOTA和mMOTA之间获得平衡。IBDV标准在第4.3.2节中，我们引入了帧间最佳匹配距离向量（IBDV）作为帧率嵌入的特征。我们进一步设计了更多不同的匹配规则，以了解这些规则如何影响整体结果。我们在表中展示了三种不同的匹配规则。'random'意味着我们随机采样对作为匹配对。'sim.'意味着我们选择外观相似性最高的对作为匹配对。'dist.'意味着我们选择位置距离最近的对作为匹配对。结果表明，' sim.'和'dist.'规则的表现优于'random'规则，而'dist.'规则略优于'sim.'规则。这可能是因为位置信息通常是更可靠的。它也提醒我们，更复杂的规则可能更有效。

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/5e385319a9b943b6bfe6a8467f717e80.png" width="800" />
</div>

![在这里插入图片描述]()


### 5.4.6 可视化和分析
在本节中，我们通过可视化和分析展示了数据模拟和我们提出的方法。

**FraMOT模拟数据的可视化**  图8展示了来自序列MOT17-13和序列MOT20-03的一些选定的模拟图像数据。在MOT17-13中，当k = 1时，所关注的靶标（ID 133）在相邻帧之间的运动很小，而当k = 50时，运动变得大得多。在MOT20-03中，靶标（ID 34和95）的运动适度，但在这种拥挤的场景中，匹配候选者的数量仍然大大增加。图9展示了在匹配过程中候选者数量关于采样因子k和阈值因子r的曲线，即有多少可能的候选者与关注对象的距离相等或更近，与r倍距离到正确真实对象的距离相比。换句话说，这些曲线显示了在不同k和不同阈值策略下，我们必须比较的平均对象数量。通常，r设置为大于1的数字，以确保具有相同身份编号的正确对象被比较，或者我们无法回忆起之前跟踪的对象。如果候选者数量很大，那意味着我们犯错的机会更多，因此任务更具挑战性。从两个图中我们可以看到，当k增加时，候选者数量显著增加，特别是在MOT20数据集中。事实上，在正常帧率场景中，候选者数量略大于1，这意味着在大多数情况下，找到具有最近距离的对象将命中真实情况。这也很好地解释了为什么在正常帧率下引入YOLO-X基线中的Re-ID分支并没有提高结果，因为位置信息非常可靠以执行匹配。图9显示，由于不同的候选者数量，FraMOT任务变得更加困难。

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/14d1c14d2304487a8a62193f1863854e.png" width="800" />
</div>

![在这里插入图片描述]()


<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/b509992470f2434dac31049f45aa9b27.png" width="800" />
</div>

![在这里插入图片描述]()


**PTS数据的可视化**  图10可视化了本文中使用的关联特征。我们在k = 1和k = 50的采样率下都展示了PTS和非PTS关联特征。四个子图显示，在应用PTS训练后，相同和不同身份对之间的边界更清晰，这意味着任务更容易。在两种采样率下，非PTS对应物中都有更多的困难数据点。我们可以进一步发现，在k = 50时，PTS和非PTS之间的差距比k = 1时扩大了，这表明PTS在较大的采样率下更有效。

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/629a076da23241c4a9410084c1d2ebe5.png" width="800" />
</div>
![在这里插入图片描述]()



**帧率不敏感关联模块的注意力嵌入的可视化** 图11展示了所提出的帧率不敏感关联模块（FAAM）在不同采样率（不同的k）下学习到的注意力图。当k = 1时，注意力图的通道按升序排序。其他注意力图与k = 1的注意力图对齐。当k较小时，例如k < 16，关联特征的右部分被强调。然后，当k变大时，右部分变得不那么重要。同时，一些关键点在左部分被关注。这些观察结果反映了模块倾向于在不同的采样率下关注关联特征的不同维度。我们假设大部分右部与运动特征有关，这很好地解释了当帧率降低时，运动变得不那么可靠。更有趣的是，当帧率略低时（例如，k = 8），注意力图关注了更多不同的通道，这表明运动线索仍然是可靠的，但可能不可靠使用单一的运动特征（例如，IoU）。

<div align=center>
   <img src="https://img-blog.csdnimg.cn/direct/aca8c8665e7d4f5992b9d2b784132427.png" width="800" />
</div>
![在这里插入图片描述]()


# 6 结论和未来工作
在本文中，我们引入了帧率不敏感的多目标跟踪（FraMOT）作为经典MOT任务的扩展挑战，以寻求对跟踪问题的更智能解决方案。我们还介绍了我们通过帧率不敏感MOT框架和周期性训练方案（FAPS）来解决这一新挑战的初步尝试。在提出的框架中，精心设计的帧率不敏感关联模块（FAAM）能够自动推断帧率信息，并利用它来帮助更复杂的运动-外观关系的识别匹配；而周期性训练方案（PTS）对齐了训练和推理环境，有助于减少FraMOT中训练和推理之间的扩大差距。这两种提出的方法成功避免了输入帧率变化时跟踪性能的急剧下降，并有助于构建更稳健的跟踪算法。实验结果和分析表明，所提出的方法有效。在未来，我们可能会继续关注如何进一步提高对各种帧率的跟踪稳定性。核心问题可能是如何开发更好的帧率感知方法，以充分利用视频信息来辅助任务。此外，FraMOT使得利用巧妙的策略获得改进变得更加困难。我们可能会找到在多个帧率下工作而不是仅在正常帧率下工作的更稳健的策略，从而提高跟踪的鲁棒性。

# 声明
本文内容为论文学习收获分享，受限于知识能力，本文对原文的理解可能存在偏差，最终内容以原论文为准备。  
本文信息旨在传播和交流学术，其内容由作者负责，不代表本号观点。文中内容如涉及作品文字。图片等内容、版权和其他问题，请及时与我们联系，我们将在第一时间删文处理。
